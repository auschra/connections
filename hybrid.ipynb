{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to /home/auschra/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import nltk\n",
    "from nltk.corpus import wordnet as wn\n",
    "import json\n",
    "from itertools import combinations\n",
    "\n",
    "nltk.download('wordnet')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# word form -> (morphology, orthography, phonology and multi word expressions)\n",
    "#       properties of the word and characters themselvies. ie. silent letters, sounds like, pre/suffix\n",
    "\n",
    "# PhonologicalEncoder() To-do: phonemes, (phonology)\n",
    "\n",
    "# OrthographicEncoder() To-do: graphemes, (orthography)\n",
    "\n",
    "# MorphologicalEncoder() To-do: morphemes, (morphology)\n",
    "\n",
    "# LexicalEncoder() To-do: lemmas, (lexicon)\n",
    "\n",
    "# SemanticEncoder() done: synonyms, hyponyms/hypernyms,(homographs)\n",
    "        # To-do:  meronyms/holonyms, polysemy (maybe?)\n",
    "\n",
    "# SyntacticEncoder() To-do: word order, word class, (morphology)\n",
    "\n",
    "# PragmaticEncoder() To-do: implicature, presupposition, (conversational implicature)\n",
    "\n",
    "# DiscourseEncoder() To-do: coherence, cohesion, (anaphora)\n",
    "\n",
    "# WorldEncoder() To-do: encyclopedic, (associations)\n",
    "\n",
    "# Combinatorial() To-do: combinations\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = \"\"\"Solve today’s NYT Connections game. Here are the instructions for how to play this game:\n",
    "Find groups of four items that share something in common.\n",
    "Category Examples:\n",
    "FISH: Bass, Flounder, Salmon, Trout\n",
    "FIRE ___: Ant, Drill, Island, Opal\n",
    "Categories will always be more specific than\n",
    "‘5-LETTER-WORDS’, ‘NAMES’, or ‘VERBS.’\n",
    "Example 1:\n",
    "Words: [‘DART’, ‘HEM’, ‘PLEAT’, ‘SEAM’,\n",
    "‘CAN’, ‘CURE’, ‘DRY’, ‘FREEZE’, ‘BITE’,\n",
    "‘EDGE’, ‘PUNCH’, ‘SPICE’, ‘CONDO’, ‘HAW’,\n",
    "‘HERO’, ‘LOO’]\n",
    "Groupings:\n",
    "1. Things to sew: [‘DART’, ‘HEM’, ‘PLEAT’,\n",
    "‘SEAM’]\n",
    "2. Ways to preserve food: [‘CAN’, ‘CURE’,\n",
    "‘DRY’, ‘FREEZE’]\n",
    "3. Sharp quality: [‘BITE’, ‘EDGE’, ‘PUNCH’,\n",
    "‘SPICE’]\n",
    "4. Birds minus last letter: [‘CONDO’, ‘HAW’,\n",
    "‘HERO’, ‘LOO’]\n",
    "Example 2:\n",
    "Words: [1COLLECTIVE’, ‘COMMON’, ‘JOINT’,\n",
    "‘MUTUAL’, ‘CLEAR’, ‘DRAIN’, ‘EMPTY’,\n",
    "‘FLUSH’, ‘CIGARETTE’, ‘PENCIL’, ‘TICKET’,\n",
    "‘TOE’, ‘AMERICAN’, ‘FEVER’, ‘LUCID’,\n",
    "‘PIPE’]\n",
    "Groupings:\n",
    "1. Shared: [‘COLLECTIVE’, ‘COMMON’,\n",
    "‘JOINT’, ‘MUTUAL’]\n",
    "2. Rid of contents: [‘CLEAR’, ‘DRAIN’,\n",
    "‘EMPTY’, ‘FLUSH’]\n",
    "3. Associated with “stub”: [‘CIGARETTE’,\n",
    "‘PENCIL’, ‘TICKET’, ‘TOE’]\n",
    "4. __ Dream: [‘AMERICAN’, ‘FEVER’, ‘LU-\n",
    "CID’, ‘PIPE’])\n",
    "Example 3:\n",
    "Words: [‘HANGAR’, ‘RUNWAY’, ‘TARMAC’,\n",
    "‘TERMINAL’, ‘ACTION’, ‘CLAIM’, ‘COM-\n",
    "PLAINT’, ‘LAWSUIT’, ‘BEANBAG’, ‘CLUB’,\n",
    "‘RING’, ‘TORCH’, ‘FOXGLOVE’, ‘GUMSHOE’,\n",
    "‘TURNCOAT’, ‘WINDSOCK’]\n",
    "Groupings:\n",
    "1. Parts of an airport: [‘HANGAR’, ‘RUNWAY’,\n",
    "‘TARMAC’, ‘TERMINAL’]\n",
    "2. Legal terms: [‘ACTION’, ‘CLAIM’, ‘COM-\n",
    "PLAINT’, ‘LAWSUIT’]\n",
    "3. Things a juggler juggles: [‘BEANBAG’,\n",
    "‘CLUB’, ‘RING’, ‘TORCH’]\n",
    "4. Words ending in clothing: [‘FOXGLOVE’,\n",
    "‘GUMSHOE’, ‘TURNCOAT’, ‘WIND-\n",
    "SOCK’]\n",
    "Categories share commonalities:\n",
    "• There are 4 categories of 4 words each\n",
    "• Every word will be in only 1 category\n",
    "• One word will never be in two categories\n",
    "• As the category number increases, the connec-\n",
    "tions between the words and their category\n",
    "become more obscure. Category 1 is the most\n",
    "easy and intuitive and Category 4 is the hard-\n",
    "est\n",
    "• There may be a red herrings (words that seems\n",
    "to belong together but actually are in separate\n",
    "categories)\n",
    "• Category 4 often contains compound words\n",
    "with a common prefix or suffix word\n",
    "• A few other common categories include word\n",
    "and letter patterns, pop culture clues (such as\n",
    "music and movie titles) and fill-in-the-blank\n",
    "phrases\n",
    "You will be given a new example (Example 4) with\n",
    "today’s list of words. First explain your reason\n",
    "for each category and then give your final answer\n",
    "following the structure below (Replace Category 1,\n",
    "2, 3, 4 with their names instead)\n",
    "Groupings:\n",
    "Category1: [word1, word2, word3, word4]\n",
    "Category2: [word5, word6, word7, word8]\n",
    "Category3: [word9, word10, word11, word12]\n",
    "Category4: [word13, word14, word15, word16]\n",
    "Remember that the same word cannot be re-\n",
    "peated across multiple categories, and you need\n",
    "to output 4 categories with 4 distinct words each.\n",
    "Also do not make up words not in the list. This is\n",
    "the most important rule. Please obey\n",
    "Example 4:\n",
    "Words : [InsertGame]\n",
    "Grouping\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# example \n",
    "# Words: [‘DART’, ‘HEM’, ‘PLEAT’, ‘SEAM’, ‘CAN’, ‘CURE’, ‘DRY’, ‘FREEZE’, ‘BITE’, ‘EDGE’, ‘PUNCH’, ‘SPICE’, ‘CONDO’, ‘HAW’, ‘HERO’, ‘LOO’]\n",
    "# Groupings:\n",
    "# 1. Things to sew: [‘DART’, ‘HEM’, ‘PLEAT’, ‘SEAM’]\n",
    "# 2. Ways to preserve food: [‘CAN’, ‘CURE’, ‘DRY’, ‘FREEZE’]\n",
    "# 3. Sharp quality: [‘BITE’, ‘EDGE’, ‘PUNCH’, ‘SPICE’]\n",
    "# 4. Birds minus last letter: [‘CONDO’, ‘HAW’, ‘HERO’, ‘LOO’]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define semantics class -> groups words by \n",
    "\n",
    "class Semantics():\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.min_similarity = 0.0\n",
    "        self.history = []\n",
    "        self.words = []\n",
    "        self.groups = []\n",
    "\n",
    "    def load_history(self, path):\n",
    "        with open(path , 'r') as f:\n",
    "            self.history = json.load(f)\n",
    "\n",
    "    # synonyms / homonym\n",
    "    def get_synsets(self, word):                                \n",
    "        return wn.synsets(word)\n",
    "    \n",
    "    # find the best synonyms for each word to be compared\n",
    "    def get_best_pair_similarity(self, word1, word2):           \n",
    "        syns1 = self.get_synsets(word1)\n",
    "        syns2 = self.get_synsets(word2)\n",
    "        highest_similarity = 0\n",
    "        \n",
    "        for syn1 in syns1:\n",
    "            for syn2 in syns2:\n",
    "                similarity = syn1.wup_similarity(syn2)\n",
    "                if similarity and similarity > highest_similarity:\n",
    "                    highest_similarity = similarity\n",
    "                    \n",
    "        return highest_similarity if highest_similarity >= self.min_similarity else None\n",
    "        \n",
    "     # find best group of 4 \n",
    "    def find_best_word_groups(self, words, group_size=4):          \n",
    "        if len(words) < group_size:\n",
    "            return []\n",
    "            \n",
    "        # find cossim between words in list\n",
    "        similarities = {}\n",
    "        for word1, word2 in combinations(words, 2):\n",
    "            sim = self.get_best_pair_similarity(word1, word2)\n",
    "            if sim:\n",
    "                similarities[(word1, word2)] = sim\n",
    "                \n",
    "        # find group of 4 with best average similarity\n",
    "        best_group = None\n",
    "        best_group_score = 0\n",
    "        \n",
    "        for word_group in combinations(words, group_size):\n",
    "            group_pairs = list(combinations(word_group, 2))\n",
    "            if all(pair in similarities for pair in group_pairs):\n",
    "                group_score = sum(similarities[pair] for pair in group_pairs) / len(group_pairs)\n",
    "                if group_score > best_group_score:\n",
    "                    best_group_score = group_score\n",
    "                    best_group = word_group\n",
    "                    \n",
    "\n",
    "        # recursively find best group with remaining words\n",
    "        if best_group:\n",
    "            print(f\"Found group with average similarity {best_group_score:.3f}:\", best_group)                                                                      \n",
    "            remaining_words = [w for w in words if w not in best_group]\n",
    "            return [best_group] + self.find_best_word_groups(remaining_words, group_size)\n",
    "        else:\n",
    "            print(f\"No group of {group_size} words found.\")\n",
    "            return []\n",
    "            \n",
    "    # process each entry\n",
    "    def process(self, entry):             \n",
    "        self.groups = entry['answers']\n",
    "        self.words = [word.lower() for group in self.groups for word in group['members']]\n",
    "        print(\"\\nProcessing words:\", self.words)\n",
    "        \n",
    "        word_groups = self.find_best_word_groups(self.words)\n",
    "        return word_groups\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Processing words: ['hail', 'rain', 'sleet', 'snow', 'bucks', 'heat', 'jazz', 'nets', 'option', 'return', 'shift', 'tab', 'kayak', 'level', 'mom', 'racecar']\n",
      "Found group with average similarity 0.900: ('hail', 'rain', 'sleet', 'snow')\n",
      "Found group with average similarity 0.830: ('option', 'return', 'shift', 'tab')\n",
      "Found group with average similarity 0.692: ('bucks', 'heat', 'nets', 'level')\n",
      "No group of 4 words found.\n"
     ]
    }
   ],
   "source": [
    "# run\n",
    "sem = Semantics()\n",
    "sem.load_history('datasets/history.json')\n",
    "results = sem.process(sem.history[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package cmudict to /home/auschra/nltk_data...\n",
      "[nltk_data]   Unzipping corpora/cmudict.zip.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['HH', 'AH0', 'L', 'OW1'], ['HH', 'EH0', 'L', 'OW1']]\n"
     ]
    }
   ],
   "source": [
    "# phonology test\n",
    "from nltk.corpus import cmudict\n",
    "nltk.download('cmudict')\n",
    "cmu = cmudict.dict()\n",
    "\n",
    "for entry in history[:1]:\n",
    "    groups = entry['answers']       # entry is a single game\n",
    "    words = [word for group in groups for word in group['members']]    # get all words \n",
    "    words = [word.lower() for word in words]\n",
    "    for word in words:\n",
    "        phoneme = cmu[']\n",
    "\n",
    "print(phoneme)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class Phonology():\n",
    "    def __init__(self):\n",
    "        super().__init__()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n"
     ]
    }
   ],
   "source": [
    "word = 'racecar'\n",
    "print(word == word[::-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Anagram found in group: 94 ['open', 'peon', 'pone', 'nepo']\n"
     ]
    }
   ],
   "source": [
    "class Orthography(): # anagram, palindrome, alliteration\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "\n",
    "# anagram (single use in archive lol)\n",
    "for entry in history:\n",
    "    groups = entry['answers']       # entry is a single game\n",
    "    words = [word for group in groups for word in group['members']]    # get all words \n",
    "    words = [word.lower() for word in words]\n",
    "\n",
    "    an_count = 0\n",
    "    angrams = []\n",
    "    palindrone_count = 0\n",
    "    palindrome = []\n",
    "    # never this simple\n",
    "    # alliteration = []\n",
    "\n",
    "    for i, word in enumerate(words):\n",
    "        for j, other_word in enumerate(words):\n",
    "            if i == j:  # skip same word \n",
    "                continue\n",
    "            if sorted(word) == sorted(other_word):\n",
    "                an_count += 1\n",
    "                angrams.append(other_word)\n",
    "            elif word == word[::-1]:\n",
    "                palindrone_count += 1\n",
    "                palindrome.append(word)\n",
    "            else:\n",
    "                pass\n",
    "\n",
    "        if an_count == 4:\n",
    "            print(f\"Anagram found in group: {entry['id']} {angrams}\")\n",
    "\n",
    "        if palindrone_count == 2:\n",
    "            print(f\"Palindrome found in group: {entry['id']} {palindrome}\")\n",
    "\n",
    "    an_count = 0\n",
    "    palindrone_count = 0\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PatternMatcher():\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "\n",
    "\n",
    "class Morphology():\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "\n",
    "\n",
    "\n",
    "class MultiWord():\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "\n",
    "class Encyclopedic():\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "\n",
    "class Association():\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "\n",
    "class Combinations():\n",
    "    def __init__(self):\n",
    "        super().__init__()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ConnectionsEncoder(nn.Module):\n",
    "    def __init__(self, embd=768):\n",
    "        super().__init__()\n",
    "            self.model = 'Llama-3-1-8B'\n",
    "            self.semantic_encoder = Semantics()\n",
    "            self.pattern_matcher = PatternMatcher()\n",
    "            self.morphology = Morphology()\n",
    "            self.orthography = Orthography()\n",
    "            self.phonology = Phonology()\n",
    "            self.multi_word = MultiWord()\n",
    "            self.encyclopedic = Encyclopedic()\n",
    "            self.association = Association()\n",
    "            self.combinations = Combinations()\n",
    "\n",
    "\n",
    "    def relationships(self, words):\n",
    "\n",
    "        prompt=\"\"\"\n",
    "        Consider these words and their semantic relationships.\n",
    "        Look for:\n",
    "        1. Direct category membership\n",
    "        2. Metaphorical connections\n",
    "        3. Word play or double meanings\n",
    "        4. Context-dependent relationships\n",
    "        \"\"\"\n",
    "\n",
    "    return self.combine_evidence(semantic_embeddings, patterns)\n",
    "\n",
    "    # beam search to find optimal groupings\n",
    "    # should 1 shot the problem, don't fall for red herrings\n",
    "\n",
    "    def combine_evidence(self, semantic_embeddings, patterns):\n",
    "        # combine all evidence\n",
    "        return groupings\n",
    "    \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BeamSearch(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.encoder = ConnectionsEncoder()\n",
    "\n",
    "    # beam search to find optimal groupings\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
