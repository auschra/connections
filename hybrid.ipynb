{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to /home/auschra/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import nltk\n",
    "from nltk.corpus import wordnet as wn\n",
    "import json\n",
    "from itertools import combinations\n",
    "\n",
    "nltk.download('wordnet')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# word form -> (morphology, orthography, phonology and multi word expressions)\n",
    "#       properties of the word and characters themselvies. ie. silent letters, sounds like, pre/suffix\n",
    "\n",
    "# PhonologicalEncoder() To-do: phonemes, (phonology)\n",
    "\n",
    "# OrthographicEncoder() To-do: graphemes, (orthography)\n",
    "\n",
    "# MorphologicalEncoder() To-do: morphemes, (morphology)\n",
    "\n",
    "# LexicalEncoder() To-do: lemmas, (lexicon)\n",
    "\n",
    "# SemanticEncoder() done: synonyms, hyponyms/hypernyms,(homographs)\n",
    "        # To-do:  meronyms/holonyms, polysemy (maybe?)\n",
    "\n",
    "# SyntacticEncoder() To-do: word order, word class, (morphology)\n",
    "\n",
    "# PragmaticEncoder() To-do: implicature, presupposition, (conversational implicature)\n",
    "\n",
    "# DiscourseEncoder() To-do: coherence, cohesion, (anaphora)\n",
    "\n",
    "# WorldEncoder() To-do: encyclopedic, (associations)\n",
    "\n",
    "# Combinatorial() To-do: combinations\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = \"\"\"Solve today’s NYT Connections game. Here are the instructions for how to play this game:\n",
    "Find groups of four items that share something in common.\n",
    "Category Examples:\n",
    "FISH: Bass, Flounder, Salmon, Trout\n",
    "FIRE ___: Ant, Drill, Island, Opal\n",
    "Categories will always be more specific than\n",
    "‘5-LETTER-WORDS’, ‘NAMES’, or ‘VERBS.’\n",
    "Example 1:\n",
    "Words: [‘DART’, ‘HEM’, ‘PLEAT’, ‘SEAM’,\n",
    "‘CAN’, ‘CURE’, ‘DRY’, ‘FREEZE’, ‘BITE’,\n",
    "‘EDGE’, ‘PUNCH’, ‘SPICE’, ‘CONDO’, ‘HAW’,\n",
    "‘HERO’, ‘LOO’]\n",
    "Groupings:\n",
    "1. Things to sew: [‘DART’, ‘HEM’, ‘PLEAT’,\n",
    "‘SEAM’]\n",
    "2. Ways to preserve food: [‘CAN’, ‘CURE’,\n",
    "‘DRY’, ‘FREEZE’]\n",
    "3. Sharp quality: [‘BITE’, ‘EDGE’, ‘PUNCH’,\n",
    "‘SPICE’]\n",
    "4. Birds minus last letter: [‘CONDO’, ‘HAW’,\n",
    "‘HERO’, ‘LOO’]\n",
    "Example 2:\n",
    "Words: [1COLLECTIVE’, ‘COMMON’, ‘JOINT’,\n",
    "‘MUTUAL’, ‘CLEAR’, ‘DRAIN’, ‘EMPTY’,\n",
    "‘FLUSH’, ‘CIGARETTE’, ‘PENCIL’, ‘TICKET’,\n",
    "‘TOE’, ‘AMERICAN’, ‘FEVER’, ‘LUCID’,\n",
    "‘PIPE’]\n",
    "Groupings:\n",
    "1. Shared: [‘COLLECTIVE’, ‘COMMON’,\n",
    "‘JOINT’, ‘MUTUAL’]\n",
    "2. Rid of contents: [‘CLEAR’, ‘DRAIN’,\n",
    "‘EMPTY’, ‘FLUSH’]\n",
    "3. Associated with “stub”: [‘CIGARETTE’,\n",
    "‘PENCIL’, ‘TICKET’, ‘TOE’]\n",
    "4. __ Dream: [‘AMERICAN’, ‘FEVER’, ‘LU-\n",
    "CID’, ‘PIPE’])\n",
    "Example 3:\n",
    "Words: [‘HANGAR’, ‘RUNWAY’, ‘TARMAC’,\n",
    "‘TERMINAL’, ‘ACTION’, ‘CLAIM’, ‘COM-\n",
    "PLAINT’, ‘LAWSUIT’, ‘BEANBAG’, ‘CLUB’,\n",
    "‘RING’, ‘TORCH’, ‘FOXGLOVE’, ‘GUMSHOE’,\n",
    "‘TURNCOAT’, ‘WINDSOCK’]\n",
    "Groupings:\n",
    "1. Parts of an airport: [‘HANGAR’, ‘RUNWAY’,\n",
    "‘TARMAC’, ‘TERMINAL’]\n",
    "2. Legal terms: [‘ACTION’, ‘CLAIM’, ‘COM-\n",
    "PLAINT’, ‘LAWSUIT’]\n",
    "3. Things a juggler juggles: [‘BEANBAG’,\n",
    "‘CLUB’, ‘RING’, ‘TORCH’]\n",
    "4. Words ending in clothing: [‘FOXGLOVE’,\n",
    "‘GUMSHOE’, ‘TURNCOAT’, ‘WIND-\n",
    "SOCK’]\n",
    "Categories share commonalities:\n",
    "• There are 4 categories of 4 words each\n",
    "• Every word will be in only 1 category\n",
    "• One word will never be in two categories\n",
    "• As the category number increases, the connec-\n",
    "tions between the words and their category\n",
    "become more obscure. Category 1 is the most\n",
    "easy and intuitive and Category 4 is the hard-\n",
    "est\n",
    "• There may be a red herrings (words that seems\n",
    "to belong together but actually are in separate\n",
    "categories)\n",
    "• Category 4 often contains compound words\n",
    "with a common prefix or suffix word\n",
    "• A few other common categories include word\n",
    "and letter patterns, pop culture clues (such as\n",
    "music and movie titles) and fill-in-the-blank\n",
    "phrases\n",
    "You will be given a new example (Example 4) with\n",
    "today’s list of words. First explain your reason\n",
    "for each category and then give your final answer\n",
    "following the structure below (Replace Category 1,\n",
    "2, 3, 4 with their names instead)\n",
    "Groupings:\n",
    "Category1: [word1, word2, word3, word4]\n",
    "Category2: [word5, word6, word7, word8]\n",
    "Category3: [word9, word10, word11, word12]\n",
    "Category4: [word13, word14, word15, word16]\n",
    "Remember that the same word cannot be re-\n",
    "peated across multiple categories, and you need\n",
    "to output 4 categories with 4 distinct words each.\n",
    "Also do not make up words not in the list. This is\n",
    "the most important rule. Please obey\n",
    "Example 4:\n",
    "Words : [InsertGame]\n",
    "Grouping\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# example \n",
    "# Words: [‘DART’, ‘HEM’, ‘PLEAT’, ‘SEAM’, ‘CAN’, ‘CURE’, ‘DRY’, ‘FREEZE’, ‘BITE’, ‘EDGE’, ‘PUNCH’, ‘SPICE’, ‘CONDO’, ‘HAW’, ‘HERO’, ‘LOO’]\n",
    "# Groupings:\n",
    "# 1. Things to sew: [‘DART’, ‘HEM’, ‘PLEAT’, ‘SEAM’]\n",
    "# 2. Ways to preserve food: [‘CAN’, ‘CURE’, ‘DRY’, ‘FREEZE’]\n",
    "# 3. Sharp quality: [‘BITE’, ‘EDGE’, ‘PUNCH’, ‘SPICE’]\n",
    "# 4. Birds minus last letter: [‘CONDO’, ‘HAW’, ‘HERO’, ‘LOO’]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [],
   "source": [
    "# semantics class\n",
    "\n",
    "class Semantics():\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.min_similarity = 0.0\n",
    "        self.history = []\n",
    "        self.words = []\n",
    "        self.groups = []\n",
    "        self.entries = 0\n",
    "        self.correct = 0 \n",
    "\n",
    "    @property\n",
    "    def accuracy(self):\n",
    "        return (self.correct / self.entries) if self.entries > 0 else 0\n",
    "        \n",
    "    def load_history(self, path):\n",
    "        with open(path , 'r') as f:\n",
    "            self.history = json.load(f)\n",
    "\n",
    "    # synonyms / homonym\n",
    "    def get_synsets(self, word):                                \n",
    "        return wn.synsets(word)\n",
    "    \n",
    "    # find the best synonyms for each word to be compared\n",
    "    def get_best_pair_similarity(self, word1, word2):           \n",
    "        syns1 = self.get_synsets(word1)\n",
    "        syns2 = self.get_synsets(word2)\n",
    "        highest_similarity = 0\n",
    "        \n",
    "        for syn1 in syns1:\n",
    "            for syn2 in syns2:\n",
    "                similarity = syn1.wup_similarity(syn2)\n",
    "                if similarity and similarity > highest_similarity:\n",
    "                    highest_similarity = similarity\n",
    "                    \n",
    "        return highest_similarity if highest_similarity >= self.min_similarity else None\n",
    "        \n",
    "     # find best group of 4 \n",
    "    def find_best_word_groups(self, words, group_size=4):          \n",
    "        if len(words) < group_size:\n",
    "            return []\n",
    "            \n",
    "        # find cossim between words in list\n",
    "        similarities = {}\n",
    "        for word1, word2 in combinations(words, 2):\n",
    "            sim = self.get_best_pair_similarity(word1, word2)\n",
    "            if sim:\n",
    "                similarities[(word1, word2)] = sim\n",
    "                \n",
    "        # find group of 4 with best average similarity\n",
    "        best_group = None\n",
    "        best_group_score = 0\n",
    "        \n",
    "        for word_group in combinations(words, group_size):\n",
    "            group_pairs = list(combinations(word_group, 2))\n",
    "            if all(pair in similarities for pair in group_pairs):\n",
    "                group_score = sum(similarities[pair] for pair in group_pairs) / len(group_pairs)\n",
    "                if group_score > best_group_score:\n",
    "                    best_group_score = group_score\n",
    "                    best_group = word_group\n",
    "                    \n",
    "        # if cossim not found, likely not synset for that word \n",
    "        if not best_group:\n",
    "            best_group = tuple(words[:group_size])\n",
    "\n",
    "        else:\n",
    "            pass\n",
    "            \n",
    "        # recursively find best group with remaining words\n",
    "        remaining_words = [w for w in words if w not in best_group]\n",
    "        return [best_group] + self.find_best_word_groups(remaining_words, group_size)\n",
    "\n",
    "            \n",
    "    # compare predicted with actual\n",
    "    def compare_groups(self, predicted_groups, actual_groups):\n",
    "        if not predicted_groups:\n",
    "            return 0\n",
    "            \n",
    "        # convert everything to lower case\n",
    "        actual_group_sets = [set(word.lower() for word in group['members']) for group in actual_groups]\n",
    "        predicted_group_sets = [set(word.lower() for word in group) for group in predicted_groups]\n",
    "        \n",
    "        correct_groups = 0\n",
    "        matched_actual_groups = set()\n",
    "        \n",
    "        # check pred vs actual, and that not already in matched\n",
    "        for pred_group in predicted_group_sets:\n",
    "            for i, actual_group in enumerate(actual_group_sets):\n",
    "                if i not in matched_actual_groups and pred_group == actual_group:\n",
    "                    print(f\"correct: {pred_group}\")\n",
    "                    correct_groups += 1\n",
    "                    matched_actual_groups.add(i)\n",
    "                    break\n",
    "\n",
    "        return correct_groups\n",
    "\n",
    "    # process single entry\n",
    "    def process(self, entry):\n",
    "        self.groups = entry['answers']\n",
    "        self.words = [word.lower() for group in self.groups for word in group['members']]\n",
    "        print(\"\\n words:\", self.words)\n",
    "        \n",
    "        word_groups = self.find_best_word_groups(self.words)\n",
    "        correct_groups = self.compare_groups(word_groups, self.groups)\n",
    "        \n",
    "        # update metrics\n",
    "        self.correct += correct_groups\n",
    "        self.entries += len(self.groups)  \n",
    "\n",
    "        # running metrics per entry\n",
    "        print(f'Correct groups in this entry: {correct_groups} out of {len(self.groups)}')\n",
    "        print('Predicted:', word_groups)\n",
    "        print('Actual:', [[word.lower() for word in group['members']] for group in self.groups])\n",
    "        \n",
    "        return word_groups, self.accuracy\n",
    "    \n",
    "    # process whole history\n",
    "    def process_all_entries(self):\n",
    "        print(\"\\nProcessing all history entries...\")\n",
    "        all_results = []\n",
    "        \n",
    "        for i, entry in enumerate(self.history):\n",
    "            print(f\"\\nEntry {i+1}/{len(self.history)}\")\n",
    "            word_groups, current_accuracy = self.process(entry)\n",
    "            all_results.append({\n",
    "                'entry_index': i,\n",
    "                'predicted_groups': word_groups,\n",
    "                'actual_groups': [[word.lower() for word in group['members']] for group in entry['answers']],\n",
    "                'correct_groups': self.correct - sum(result.get('correct_groups', 0) for result in all_results),\n",
    "                'accuracy_so_far': current_accuracy\n",
    "            })\n",
    "            \n",
    "        # final stats\n",
    "        print(\"\\nFinal Statistics:\")\n",
    "        print(f\"Total entries processed: {len(self.history)}\")\n",
    "        print(f\"Total groups processed: {self.entries}\")\n",
    "        print(f\"Total correct groups: {self.correct}\")\n",
    "        print(f\"Final accuracy: {self.accuracy:.2%}\")\n",
    "        \n",
    "        return all_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sem = Semantics()\n",
    "sem.load_history('datasets/history.json')\n",
    "all_results = sem.process_all_entries()\n",
    "\n",
    "# all results \n",
    "stats = False\n",
    "if stats:\n",
    "    for result in all_results:\n",
    "        print(f\"\\nEntry {result['entry_index']}:\")\n",
    "        print(f\"Correct groups: {result['correct_groups']}\")\n",
    "        print(f\"Running accuracy: {result['accuracy_so_far']:.2%}\")\n",
    "        print(\"Predicted:\", result['predicted_groups'])\n",
    "        print(\"Actual:\", result['actual_groups'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package cmudict to /home/auschra/nltk_data...\n",
      "[nltk_data]   Unzipping corpora/cmudict.zip.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['HH', 'AH0', 'L', 'OW1'], ['HH', 'EH0', 'L', 'OW1']]\n"
     ]
    }
   ],
   "source": [
    "# phonology test\n",
    "from nltk.corpus import cmudict\n",
    "nltk.download('cmudict')\n",
    "cmu = cmudict.dict()\n",
    "\n",
    "for entry in history[:1]:\n",
    "    groups = entry['answers']       # entry is a single game\n",
    "    words = [word for group in groups for word in group['members']]    # get all words \n",
    "    words = [word.lower() for word in words]\n",
    "    for word in words:\n",
    "        phoneme = cmu[']\n",
    "\n",
    "print(phoneme)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class Phonology():\n",
    "    def __init__(self):\n",
    "        super().__init__()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n"
     ]
    }
   ],
   "source": [
    "word = 'racecar'\n",
    "print(word == word[::-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Anagram found in group: 94 ['open', 'peon', 'pone', 'nepo']\n"
     ]
    }
   ],
   "source": [
    "class Orthography(): # anagram, palindrome, alliteration\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "\n",
    "# anagram (single use in archive lol)\n",
    "for entry in history:\n",
    "    groups = entry['answers']       # entry is a single game\n",
    "    words = [word for group in groups for word in group['members']]    # get all words \n",
    "    words = [word.lower() for word in words]\n",
    "\n",
    "    an_count = 0\n",
    "    angrams = []\n",
    "    palindrone_count = 0\n",
    "    palindrome = []\n",
    "    # never this simple\n",
    "    # alliteration = []\n",
    "\n",
    "    for i, word in enumerate(words):\n",
    "        for j, other_word in enumerate(words):\n",
    "            if i == j:  # skip same word \n",
    "                continue\n",
    "            if sorted(word) == sorted(other_word):\n",
    "                an_count += 1\n",
    "                angrams.append(other_word)\n",
    "            elif word == word[::-1]:\n",
    "                palindrone_count += 1\n",
    "                palindrome.append(word)\n",
    "            else:\n",
    "                pass\n",
    "\n",
    "        if an_count == 4:\n",
    "            print(f\"Anagram found in group: {entry['id']} {angrams}\")\n",
    "\n",
    "        if palindrone_count == 2:\n",
    "            print(f\"Palindrome found in group: {entry['id']} {palindrome}\")\n",
    "\n",
    "    an_count = 0\n",
    "    palindrone_count = 0\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PatternMatcher():\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "\n",
    "\n",
    "class Morphology():\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "\n",
    "\n",
    "\n",
    "class MultiWord():\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "\n",
    "class Encyclopedic():\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "\n",
    "class Association():\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "\n",
    "class Combinations():\n",
    "    def __init__(self):\n",
    "        super().__init__()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ConnectionsEncoder(nn.Module):\n",
    "    def __init__(self, embd=768):\n",
    "        super().__init__()\n",
    "            self.model = 'Llama-3-1-8B'\n",
    "            self.semantic_encoder = Semantics()\n",
    "            self.pattern_matcher = PatternMatcher()\n",
    "            self.morphology = Morphology()\n",
    "            self.orthography = Orthography()\n",
    "            self.phonology = Phonology()\n",
    "            self.multi_word = MultiWord()\n",
    "            self.encyclopedic = Encyclopedic()\n",
    "            self.association = Association()\n",
    "            self.combinations = Combinations()\n",
    "\n",
    "\n",
    "    def relationships(self, words):\n",
    "\n",
    "        prompt=\"\"\"\n",
    "        Consider these words and their semantic relationships.\n",
    "        Look for:\n",
    "        1. Direct category membership\n",
    "        2. Metaphorical connections\n",
    "        3. Word play or double meanings\n",
    "        4. Context-dependent relationships\n",
    "        \"\"\"\n",
    "\n",
    "    return self.combine_evidence(semantic_embeddings, patterns)\n",
    "\n",
    "    # beam search to find optimal groupings\n",
    "    # should 1 shot the problem, don't fall for red herrings\n",
    "\n",
    "    def combine_evidence(self, semantic_embeddings, patterns):\n",
    "        # combine all evidence\n",
    "        return groupings\n",
    "    \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BeamSearch(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.encoder = ConnectionsEncoder()\n",
    "\n",
    "    # beam search to find optimal groupings\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
